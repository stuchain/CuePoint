# Implementation Step 7.1: Test Layers

## Implementation Overview

**What We're Building**: A comprehensive, multi-layered testing strategy that ensures code quality, reliability, and maintainability through a well-structured test pyramid. This system implements unit tests for isolated component validation, integration tests for workflow verification, GUI smoke tests for UI stability, and establishes clear guidance for test distribution across layers. The goal is to create a robust testing foundation that catches bugs early, prevents regressions, maintains fast feedback loops, and provides confidence for continuous deployment while balancing test coverage with execution speed and CI stability.

## Strategic Context and Rationale

### Why Test Layers Matter

Testing is not just about finding bugs—it's about creating a safety net that enables confident development, rapid iteration, and reliable releases. The test pyramid concept, popularized by Mike Cohn and widely adopted in the industry, provides a framework for organizing tests by their scope, speed, and cost. Understanding and implementing this hierarchy correctly is critical for sustainable software development.

**The Testing Dilemma**: Every test has trade-offs:
- **Scope**: Narrow tests (unit) are fast but may miss integration issues. Broad tests (E2E) catch real-world problems but are slow and brittle.
- **Speed**: Fast tests enable rapid feedback, but slow tests can block development velocity.
- **Reliability**: Deterministic tests are trustworthy, but flaky tests erode confidence and waste time.
- **Cost**: Writing and maintaining tests requires effort, but the cost of bugs in production is often higher.

**The Test Pyramid Solution**: By organizing tests into layers with different characteristics, we can optimize for all these concerns simultaneously:
- **Base (Unit Tests)**: Many, fast, isolated, deterministic. Catch logic errors quickly.
- **Middle (Integration Tests)**: Fewer, slower, test component interactions. Catch integration issues.
- **Top (E2E/GUI Tests)**: Fewest, slowest, test complete workflows. Catch user-facing problems.

### Industry Best Practices

The test pyramid has evolved from a simple triangle to more nuanced models:

1. **Classic Test Pyramid** (Cohn, 2009): Unit > Integration > E2E
2. **Honeycomb Model** (Hamrick, 2012): Emphasizes contract testing
3. **Testing Trophy** (Kent C. Dodds, 2018): Emphasizes integration tests
4. **Test Diamond** (Fowler, 2019): Emphasizes component tests

For CuePoint, we adopt a **pragmatic pyramid** that balances:
- **Speed**: Most tests must run in < 1 second
- **Reliability**: Tests must be deterministic and not flaky
- **Coverage**: Critical paths must be covered at multiple levels
- **Maintainability**: Tests must be easy to write, read, and update

### CuePoint-Specific Considerations

**Application Characteristics**:
- **Desktop Application**: Requires GUI testing but can avoid complex browser automation
- **Data Processing**: Heavy emphasis on parsing, matching, and transformation logic
- **Cross-Platform**: Tests must work on macOS and Windows
- **Network-Dependent**: Beatport integration requires careful test data management
- **User Workflows**: Clear, linear workflows (XML → Process → Export)

**Testing Challenges**:
1. **GUI Testing**: Qt applications can be tested, but GUI tests are inherently slower and more brittle
2. **Network Dependencies**: Beatport API calls must be mocked or recorded
3. **File I/O**: XML parsing and export generation require fixture management
4. **Platform Differences**: Some behavior differs between macOS and Windows
5. **Performance**: Processing large playlists must be benchmarked

**Our Strategy**: 
- **Heavy Unit Testing**: Most logic is pure functions (parsing, matching, filtering) → perfect for unit tests
- **Focused Integration Testing**: Test complete workflows with fixtures, avoid real network
- **Minimal GUI Testing**: Smoke tests only, verify app starts and core widgets load
- **Performance Benchmarks**: Separate from functional tests, run on schedule

## Implementation Tasks

### Task 7.1.1: Unit Test Infrastructure

**What to Build**
- Unit test framework setup and configuration
- Test discovery and execution patterns
- Mocking and fixture utilities
- Test utilities and helpers
- Code coverage tracking
- Test organization and naming conventions

**Implementation Details**

**7.1.1.1 Test Framework Selection and Configuration**

**Purpose**: Establish a robust, maintainable unit testing foundation that integrates seamlessly with the development workflow and CI/CD pipeline.

**Framework Choice: pytest**

We choose pytest over unittest for several reasons:

1. **Superior Assertions**: pytest's assertion introspection provides detailed failure messages automatically
   ```python
   # unittest: AssertionError: False is not True
   assert result == expected
   
   # pytest: AssertionError: assert 'actual' == 'expected'
   #         - actual
   #         + expected
   ```

2. **Fixtures**: Built-in fixture system is more powerful than unittest's setUp/tearDown
   ```python
   @pytest.fixture
   def sample_track():
       return Track(title="Test Track", artist="Test Artist")
   ```

3. **Parametrization**: Easy test parameterization for testing multiple inputs
   ```python
   @pytest.mark.parametrize("input,expected", [
       ("test", "TEST"),
       ("hello", "HELLO"),
   ])
   def test_uppercase(input, expected):
       assert input.upper() == expected
   ```

4. **Plugin Ecosystem**: Rich plugin ecosystem (coverage, mocking, async, etc.)
5. **Better Error Messages**: More informative error messages and tracebacks
6. **Test Discovery**: Automatic test discovery with flexible naming
7. **Markers**: Easy test categorization and selection
   ```python
   @pytest.mark.slow
   @pytest.mark.integration
   def test_complex_workflow():
       ...
   ```

**Configuration: `pytest.ini`**

```ini
[pytest]
# Test discovery patterns
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Test paths
testpaths = SRC/tests

# Output options
addopts = 
    -v
    --strict-markers
    --tb=short
    --disable-warnings
    --color=yes
    --durations=10

# Markers for test categorization
markers =
    unit: Unit tests (fast, isolated)
    integration: Integration tests (slower, test workflows)
    gui: GUI tests (slow, test UI)
    slow: Slow tests (may skip in quick runs)
    performance: Performance benchmarks
    windows: Windows-specific tests
    macos: macOS-specific tests
    network: Tests that require network (may be skipped)

# Coverage configuration
[coverage:run]
source = SRC/cuepoint
omit = 
    */tests/*
    */__pycache__/*
    */venv/*
    */build/*
    */dist/*

[coverage:report]
precision = 2
show_missing = True
skip_covered = False
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstractmethod
```

**Test Organization Structure**

```
SRC/tests/
├── conftest.py              # Shared fixtures and configuration
├── unit/                    # Unit tests (fast, isolated)
│   ├── core/               # Core logic tests
│   │   ├── test_matcher.py
│   │   ├── test_parser.py
│   │   └── test_filter.py
│   ├── data/               # Data layer tests
│   │   ├── test_rekordbox.py
│   │   └── test_beatport.py
│   ├── services/           # Service layer tests
│   │   ├── test_processor.py
│   │   └── test_export.py
│   ├── models/             # Model tests
│   │   └── test_track.py
│   └── utils/              # Utility tests
│       └── test_paths.py
├── integration/            # Integration tests (workflows)
│   ├── test_processing_flow.py
│   └── test_export_flow.py
├── ui/                     # GUI smoke tests
│   ├── test_main_window.py
│   └── test_widgets.py
└── performance/            # Performance benchmarks
    └── benchmark_processing.py
```

**7.1.1.2 Test Utilities and Helpers**

**Purpose**: Create reusable utilities that reduce test boilerplate and improve test readability and maintainability.

**Common Test Patterns in CuePoint**:

1. **Track Creation**: Many tests need sample tracks
2. **Playlist Creation**: Tests need playlists with tracks
3. **XML Fixtures**: Tests need Rekordbox XML data
4. **HTML Fixtures**: Tests need Beatport HTML responses
5. **Temporary Files**: Tests need temporary directories and files
6. **Mock Network**: Tests need to mock HTTP requests

**Implementation: `SRC/tests/conftest.py`**

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Shared pytest fixtures and test utilities.

This module provides common fixtures used across all test modules,
reducing duplication and ensuring consistency.
"""

import tempfile
from pathlib import Path
from typing import Generator
from unittest.mock import Mock, patch

import pytest

from cuepoint.models.track import Track
from cuepoint.models.playlist import Playlist


# ============================================================================
# Track and Playlist Fixtures
# ============================================================================

@pytest.fixture
def sample_track() -> Track:
    """Create a sample track for testing."""
    return Track(
        title="Test Track",
        artist="Test Artist",
        label="Test Label",
        bpm=128.0,
        key="Am",
        genre="House",
        year=2024
    )


@pytest.fixture
def sample_tracks() -> list[Track]:
    """Create multiple sample tracks."""
    return [
        Track(title=f"Track {i}", artist=f"Artist {i}", bpm=120.0 + i)
        for i in range(5)
    ]


@pytest.fixture
def sample_playlist(sample_tracks: list[Track]) -> Playlist:
    """Create a sample playlist with tracks."""
    return Playlist(
        name="Test Playlist",
        tracks=sample_tracks
    )


# ============================================================================
# File System Fixtures
# ============================================================================

@pytest.fixture
def temp_dir() -> Generator[Path, None, None]:
    """Create a temporary directory for test files."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)


@pytest.fixture
def temp_file(temp_dir: Path) -> Generator[Path, None, None]:
    """Create a temporary file path."""
    file_path = temp_dir / "test_file.txt"
    yield file_path
    if file_path.exists():
        file_path.unlink()


# ============================================================================
# XML Fixtures
# ============================================================================

@pytest.fixture
def sample_rekordbox_xml(temp_dir: Path) -> Path:
    """Create a sample Rekordbox XML file."""
    xml_content = """<?xml version="1.0" encoding="UTF-8"?>
    <DJ_PLAYLISTS Version="1.0.0">
        <PRODUCT Name="rekordbox" Version="6.7.0"/>
        <COLLECTION>
            <TRACK TrackID="1" Name="Test Track" Artist="Test Artist" BPM="128.0"/>
        </COLLECTION>
        <PLAYLISTS>
            <NODE Name="ROOT">
                <NODE Name="Test Playlist">
                    <TRACK Key="1"/>
                </NODE>
            </NODE>
        </PLAYLISTS>
    </DJ_PLAYLISTS>"""
    
    xml_path = temp_dir / "rekordbox.xml"
    xml_path.write_text(xml_content, encoding="utf-8")
    return xml_path


# ============================================================================
# Network Mocking Fixtures
# ============================================================================

@pytest.fixture
def mock_beatport_response():
    """Mock Beatport API response."""
    return {
        "tracks": [
            {
                "title": "Test Track",
                "artist": "Test Artist",
                "label": "Test Label",
                "bpm": 128.0
            }
        ]
    }


@pytest.fixture
def mock_requests_get(mock_beatport_response):
    """Mock requests.get for network tests."""
    with patch('requests.get') as mock_get:
        mock_response = Mock()
        mock_response.json.return_value = mock_beatport_response
        mock_response.status_code = 200
        mock_get.return_value = mock_response
        yield mock_get


# ============================================================================
# Service Mocking Fixtures
# ============================================================================

@pytest.fixture
def mock_cache_service():
    """Mock cache service for tests."""
    cache = Mock()
    cache.get.return_value = None
    cache.set.return_value = None
    cache.clear.return_value = None
    return cache


@pytest.fixture
def mock_logging_service():
    """Mock logging service for tests."""
    logger = Mock()
    logger.info = Mock()
    logger.warning = Mock()
    logger.error = Mock()
    logger.debug = Mock()
    return logger


# ============================================================================
# Test Markers and Utilities
# ============================================================================

def pytest_configure(config):
    """Configure pytest with custom markers."""
    config.addinivalue_line(
        "markers", "unit: Unit tests (fast, isolated)"
    )
    config.addinivalue_line(
        "markers", "integration: Integration tests (slower, test workflows)"
    )
    config.addinivalue_line(
        "markers", "gui: GUI tests (slow, test UI)"
    )
    config.addinivalue_line(
        "markers", "slow: Slow tests (may skip in quick runs)"
    )
    config.addinivalue_line(
        "markers", "performance: Performance benchmarks"
    )


# ============================================================================
# Test Helpers
# ============================================================================

def assert_tracks_equal(actual: Track, expected: Track):
    """Assert two tracks are equal, with detailed error message."""
    assert actual.title == expected.title, f"Title mismatch: {actual.title} != {expected.title}"
    assert actual.artist == expected.artist, f"Artist mismatch: {actual.artist} != {expected.artist}"
    assert actual.bpm == expected.bpm, f"BPM mismatch: {actual.bpm} != {expected.bpm}"


def create_test_xml(tracks: list[dict], temp_dir: Path) -> Path:
    """Helper to create test XML files."""
    # Implementation for creating XML from track data
    pass
```

**7.1.1.3 Unit Test Examples**

**Example 1: Pure Function Testing (Matcher)**

```python
# SRC/tests/unit/core/test_matcher.py

import pytest
from cuepoint.core.matcher import TrackMatcher
from cuepoint.models.track import Track


@pytest.mark.unit
class TestTrackMatcher:
    """Test TrackMatcher - pure function, fast unit tests."""
    
    def test_exact_match(self, sample_track: Track):
        """Test exact track matching."""
        matcher = TrackMatcher()
        query = "Test Track"
        result = matcher.match(sample_track, query)
        assert result.score == 1.0
        assert result.matched is True
    
    def test_partial_match(self, sample_track: Track):
        """Test partial track matching."""
        matcher = TrackMatcher()
        query = "Test"
        result = matcher.match(sample_track, query)
        assert result.score > 0.5
        assert result.matched is True
    
    def test_no_match(self, sample_track: Track):
        """Test no match scenario."""
        matcher = TrackMatcher()
        query = "Completely Different"
        result = matcher.match(sample_track, query)
        assert result.score == 0.0
        assert result.matched is False
    
    @pytest.mark.parametrize("query,expected_score", [
        ("Test Track", 1.0),
        ("test track", 1.0),  # Case insensitive
        ("Test", 0.7),
        ("Track", 0.7),
        ("", 0.0),
    ])
    def test_match_scores(self, sample_track: Track, query: str, expected_score: float):
        """Test various match scores."""
        matcher = TrackMatcher()
        result = matcher.match(sample_track, query)
        assert abs(result.score - expected_score) < 0.1
```

**Example 2: Parser Testing with Fixtures**

```python
# SRC/tests/unit/data/test_rekordbox.py

import pytest
from pathlib import Path
from cuepoint.data.rekordbox import RekordboxParser


@pytest.mark.unit
class TestRekordboxParser:
    """Test Rekordbox XML parser."""
    
    def test_parse_valid_xml(self, sample_rekordbox_xml: Path):
        """Test parsing valid Rekordbox XML."""
        parser = RekordboxParser()
        result = parser.parse(sample_rekordbox_xml)
        
        assert len(result.tracks) == 1
        assert result.tracks[0].title == "Test Track"
        assert result.tracks[0].artist == "Test Artist"
        assert result.tracks[0].bpm == 128.0
    
    def test_parse_invalid_xml(self, temp_dir: Path):
        """Test parsing invalid XML."""
        invalid_xml = temp_dir / "invalid.xml"
        invalid_xml.write_text("not xml content")
        
        parser = RekordboxParser()
        with pytest.raises(ValueError, match="Invalid XML"):
            parser.parse(invalid_xml)
    
    def test_parse_missing_file(self, temp_dir: Path):
        """Test parsing non-existent file."""
        parser = RekordboxParser()
        missing_file = temp_dir / "missing.xml"
        
        with pytest.raises(FileNotFoundError):
            parser.parse(missing_file)
    
    def test_parse_empty_xml(self, temp_dir: Path):
        """Test parsing empty XML."""
        empty_xml = temp_dir / "empty.xml"
        empty_xml.write_text("<?xml version='1.0'?><DJ_PLAYLISTS></DJ_PLAYLISTS>")
        
        parser = RekordboxParser()
        result = parser.parse(empty_xml)
        
        assert len(result.tracks) == 0
        assert len(result.playlists) == 0
```

**Example 3: Service Testing with Mocks**

```python
# SRC/tests/unit/services/test_processor_service.py

import pytest
from unittest.mock import Mock, patch
from cuepoint.services.processor_service import ProcessorService
from cuepoint.models.track import Track


@pytest.mark.unit
class TestProcessorService:
    """Test ProcessorService with mocked dependencies."""
    
    def test_process_track_success(
        self,
        mock_cache_service,
        mock_logging_service,
        sample_track: Track
    ):
        """Test successful track processing."""
        service = ProcessorService(
            cache_service=mock_cache_service,
            logging_service=mock_logging_service
        )
        
        result = service.process_track(sample_track)
        
        assert result is not None
        assert result.processed is True
        mock_cache_service.set.assert_called_once()
        mock_logging_service.info.assert_called()
    
    def test_process_track_with_cache_hit(
        self,
        mock_cache_service,
        mock_logging_service,
        sample_track: Track
    ):
        """Test track processing with cache hit."""
        # Setup: cache returns a result
        cached_result = Mock()
        cached_result.processed = True
        mock_cache_service.get.return_value = cached_result
        
        service = ProcessorService(
            cache_service=mock_cache_service,
            logging_service=mock_logging_service
        )
        
        result = service.process_track(sample_track)
        
        # Verify cache was checked
        mock_cache_service.get.assert_called_once()
        # Verify no processing occurred (cache hit)
        assert result == cached_result
    
    @patch('cuepoint.services.processor_service.BeatportService')
    def test_process_track_network_error(
        self,
        mock_beatport_service,
        mock_cache_service,
        mock_logging_service,
        sample_track: Track
    ):
        """Test track processing with network error."""
        # Setup: Beatport service raises exception
        mock_beatport_service.return_value.search.side_effect = ConnectionError("Network error")
        
        service = ProcessorService(
            cache_service=mock_cache_service,
            logging_service=mock_logging_service
        )
        
        with pytest.raises(ConnectionError):
            service.process_track(sample_track)
        
        # Verify error was logged
        mock_logging_service.error.assert_called()
```

### Task 7.1.2: Integration Test Infrastructure

**What to Build**
- Integration test framework setup
- Workflow test patterns
- Fixture management for integration tests
- Service integration testing
- End-to-end workflow validation

**Implementation Details**

**7.1.2.1 Integration Test Framework**

**Purpose**: Test complete workflows and component interactions without relying on external services or complex GUI automation.

**Integration Test Philosophy**:

1. **Test Real Workflows**: Test the actual user workflows (XML → Process → Export)
2. **Use Real Components**: Use real services, but mock external dependencies (network, file system where appropriate)
3. **Fast Enough**: Integration tests should complete in < 10 seconds
4. **Deterministic**: Tests must be repeatable and not flaky
5. **Isolated**: Each test should be independent and not affect others

**Integration Test Structure**:

```python
# SRC/tests/integration/test_processing_flow.py

import pytest
from pathlib import Path
from cuepoint.services.bootstrap import bootstrap_services
from cuepoint.services.processor_service import ProcessorService
from cuepoint.data.rekordbox import RekordboxParser


@pytest.mark.integration
class TestProcessingFlow:
    """Integration tests for complete processing workflow."""
    
    @pytest.fixture
    def services(self):
        """Bootstrap real services for integration testing."""
        return bootstrap_services()
    
    def test_xml_to_export_workflow(
        self,
        services,
        sample_rekordbox_xml: Path,
        temp_dir: Path
    ):
        """Test complete workflow: XML → Process → Export."""
        # Step 1: Parse XML
        parser = RekordboxParser()
        rekordbox_data = parser.parse(sample_rekordbox_xml)
        
        assert len(rekordbox_data.tracks) > 0
        assert len(rekordbox_data.playlists) > 0
        
        # Step 2: Process tracks
        processor = services['processor_service']
        playlist = rekordbox_data.playlists[0]
        
        results = []
        for track in playlist.tracks:
            result = processor.process_track(track)
            results.append(result)
        
        assert len(results) == len(playlist.tracks)
        assert all(r.processed for r in results)
        
        # Step 3: Export results
        export_service = services['export_service']
        export_path = temp_dir / "export.csv"
        
        export_service.export_results(results, export_path)
        
        assert export_path.exists()
        assert export_path.stat().st_size > 0
        
        # Step 4: Verify export content
        content = export_path.read_text()
        assert "Test Track" in content
        assert "Test Artist" in content
    
    def test_processing_with_cache(
        self,
        services,
        sample_rekordbox_xml: Path
    ):
        """Test that processing uses cache correctly."""
        processor = services['processor_service']
        parser = RekordboxParser()
        rekordbox_data = parser.parse(sample_rekordbox_xml)
        
        track = rekordbox_data.tracks[0]
        
        # First processing (cache miss)
        result1 = processor.process_track(track)
        assert result1.processed is True
        
        # Second processing (cache hit)
        result2 = processor.process_track(track)
        assert result2.processed is True
        assert result1 == result2  # Should be same result from cache
```

**7.1.2.2 Network Mocking for Integration Tests**

**Purpose**: Test workflows that involve network calls without actually calling external APIs.

**Strategy: Record and Replay**

```python
# SRC/tests/integration/test_beatport_integration.py

import pytest
from pathlib import Path
from unittest.mock import patch
from cuepoint.services.beatport_service import BeatportService


@pytest.mark.integration
@pytest.mark.network  # Can be skipped if network unavailable
class TestBeatportIntegration:
    """Integration tests for Beatport service."""
    
    @pytest.fixture
    def beatport_html_fixture(self, temp_dir: Path) -> Path:
        """Load recorded Beatport HTML response."""
        fixture_path = Path(__file__).parent / "fixtures" / "beatport_search.html"
        return fixture_path
    
    def test_search_with_recorded_response(
        self,
        beatport_html_fixture: Path,
        temp_dir: Path
    ):
        """Test Beatport search using recorded HTML response."""
        service = BeatportService()
        
        # Mock the HTTP request to return recorded HTML
        with patch('requests.get') as mock_get:
            mock_response = Mock()
            mock_response.text = beatport_html_fixture.read_text()
            mock_response.status_code = 200
            mock_get.return_value = mock_response
            
            results = service.search("Test Track")
            
            assert len(results) > 0
            assert all(r.title for r in results)
            assert all(r.artist for r in results)
```

### Task 7.1.3: GUI Smoke Test Infrastructure

**What to Build**
- GUI test framework setup
- Qt application testing utilities
- Window and widget testing patterns
- Minimal smoke test suite
- CI-friendly GUI test execution

**Implementation Details**

**7.1.3.1 GUI Testing Philosophy**

**Why Minimal GUI Testing?**:

1. **GUI Tests are Slow**: GUI tests typically take 10-100x longer than unit tests
2. **GUI Tests are Brittle**: UI changes break tests frequently, even when functionality is correct
3. **GUI Tests are Complex**: Require application startup, window management, event simulation
4. **Diminishing Returns**: Most bugs are caught by unit and integration tests
5. **CI Stability**: Flaky GUI tests can block deployments

**Our Strategy: Smoke Tests Only**

We test only the critical GUI paths:
- ✅ Application starts successfully
- ✅ Main window loads
- ✅ Core widgets instantiate
- ✅ Basic user interactions work (button clicks, file selection)

We do NOT test:
- ❌ Complex UI workflows (covered by integration tests)
- ❌ Visual appearance (manual QA)
- ❌ Edge cases in UI (covered by unit tests of underlying logic)
- ❌ Platform-specific UI behavior (manual QA)

**7.1.3.2 Qt Testing Framework**

**Framework: pytest-qt**

```python
# SRC/tests/ui/test_main_window.py

import pytest
from PySide6.QtWidgets import QApplication
from PySide6.QtCore import Qt
from cuepoint.ui.main_window import MainWindow


@pytest.mark.gui
class TestMainWindow:
    """GUI smoke tests for main window."""
    
    @pytest.fixture(scope="class")
    def qapp(self):
        """Create QApplication for GUI tests."""
        app = QApplication.instance()
        if app is None:
            app = QApplication([])
        yield app
    
    def test_main_window_creates(self, qapp):
        """Test that main window can be created."""
        window = MainWindow()
        assert window is not None
        assert window.isVisible() is False  # Not shown yet
    
    def test_main_window_shows(self, qapp, qtbot):
        """Test that main window can be shown."""
        window = MainWindow()
        window.show()
        
        qtbot.waitForWindowShown(window)
        assert window.isVisible() is True
    
    def test_main_window_widgets_exist(self, qapp, qtbot):
        """Test that core widgets exist."""
        window = MainWindow()
        window.show()
        qtbot.waitForWindowShown(window)
        
        # Verify core widgets exist
        assert window.file_selector is not None
        assert window.playlist_selector is not None
        assert window.results_view is not None
    
    def test_file_selector_button_click(self, qapp, qtbot):
        """Test file selector button interaction."""
        window = MainWindow()
        window.show()
        qtbot.waitForWindowShown(window)
        
        # Click file selector button
        button = window.file_selector.select_button
        qtbot.mouseClick(button, Qt.LeftButton)
        
        # Verify dialog appears (or file selected)
        # This is a smoke test, so we just verify no crash
        assert window.isVisible() is True
```

**7.1.3.3 CI-Friendly GUI Testing**

**Configuration for CI**:

```yaml
# .github/workflows/test.yml (excerpt)

- name: Run GUI Tests
  run: |
    # Use xvfb for headless GUI testing on Linux
    # macOS and Windows can run GUI tests directly
    if [[ "$RUNNER_OS" == "Linux" ]]; then
      xvfb-run -a pytest SRC/tests/ui/ -m gui
    else
      pytest SRC/tests/ui/ -m gui
    fi
  continue-on-error: true  # GUI tests may be flaky
```

### Task 7.1.4: Test Pyramid Guidance and Best Practices

**What to Build**
- Test distribution guidelines
- Test execution strategies
- Test maintenance practices
- Performance considerations
- Documentation and training

**Implementation Details**

**7.1.4.1 Test Distribution Guidelines**

**Target Distribution** (for CuePoint):

```
Unit Tests:        70%  (Fast, isolated, test logic)
Integration Tests: 25%  (Workflows, test interactions)
GUI Tests:          5%  (Smoke tests, verify UI loads)
```

**Decision Matrix: Which Layer?**

| Scenario | Layer | Rationale |
|----------|-------|-----------|
| Pure function (matcher, parser) | Unit | Fast, isolated, easy to test |
| Service with dependencies | Unit (mocked) | Test logic without external deps |
| Complete workflow (XML → Export) | Integration | Test real component interactions |
| UI widget logic | Unit | Test logic, not rendering |
| UI rendering | GUI (smoke) | Verify widget appears |
| Network API calls | Integration (mocked) | Test with recorded responses |
| File I/O | Integration (fixtures) | Test with real file operations |
| Performance | Benchmark | Separate performance tests |

**7.1.4.2 Test Execution Strategies**

**Fast Feedback Loop**:

```bash
# Quick test run (unit tests only, < 30 seconds)
pytest SRC/tests/unit/ -m "not slow"

# Full test run (all tests, < 5 minutes)
pytest SRC/tests/

# CI test run (all tests + coverage)
pytest SRC/tests/ --cov=SRC/cuepoint --cov-report=html
```

**Test Selection**:

```python
# Run only unit tests
pytest -m unit

# Run only integration tests
pytest -m integration

# Run only GUI tests
pytest -m gui

# Skip slow tests
pytest -m "not slow"

# Run specific test file
pytest SRC/tests/unit/core/test_matcher.py

# Run specific test
pytest SRC/tests/unit/core/test_matcher.py::TestTrackMatcher::test_exact_match
```

**7.1.4.3 Test Maintenance Best Practices**

**1. Keep Tests Simple**
- One assertion per test (when possible)
- Clear test names that describe what is tested
- Minimal setup, clear teardown

**2. Use Fixtures for Common Setup**
- Don't duplicate setup code
- Use `conftest.py` for shared fixtures
- Use fixture scoping appropriately (function, class, module, session)

**3. Test Behavior, Not Implementation**
- Test what the code does, not how it does it
- Avoid testing private methods
- Focus on public APIs

**4. Keep Tests Independent**
- Tests should not depend on each other
- Tests should be runnable in any order
- Tests should clean up after themselves

**5. Use Parametrization for Similar Tests**
- Don't duplicate test code
- Use `@pytest.mark.parametrize` for multiple inputs

**6. Mock External Dependencies**
- Mock network calls
- Mock file system (when appropriate)
- Mock time-dependent code

**7. Write Descriptive Assertions**
- Use custom assertion helpers when needed
- Provide context in assertion messages
- Use pytest's assertion introspection

## Integration Points

### CI/CD Integration

**GitHub Actions Workflow**:

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: [3.11, 3.12]
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
      
      - name: Run unit tests
        run: pytest SRC/tests/unit/ -v --cov
      
      - name: Run integration tests
        run: pytest SRC/tests/integration/ -v
      
      - name: Run GUI tests
        run: pytest SRC/tests/ui/ -m gui
        continue-on-error: true
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

### Development Workflow Integration

**Pre-commit Hooks**:

```yaml
# .pre-commit-config.yaml

repos:
  - repo: local
    hooks:
      - id: pytest-fast
        name: Run fast tests
        entry: pytest
        args: [SRC/tests/unit/, -m, "not slow", --tb=short]
        language: system
        pass_filenames: false
        always_run: true
```

## Success Metrics

### Test Coverage Goals

- **Unit Tests**: > 80% code coverage
- **Integration Tests**: Cover all critical workflows
- **GUI Tests**: Cover all critical UI paths

### Performance Goals

- **Unit Tests**: Complete in < 30 seconds
- **Integration Tests**: Complete in < 2 minutes
- **GUI Tests**: Complete in < 5 minutes
- **Full Test Suite**: Complete in < 10 minutes

### Quality Goals

- **Test Reliability**: < 1% flaky test rate
- **Test Maintenance**: < 10% of development time
- **Bug Detection**: > 90% of bugs caught before production

## Future Considerations

### Test Automation Expansion

- **Visual Regression Testing**: Screenshot comparison for UI
- **Accessibility Testing**: Automated a11y checks
- **Performance Regression Testing**: Automated performance benchmarks
- **Security Testing**: Automated security scanning

### Test Infrastructure Improvements

- **Parallel Test Execution**: Run tests in parallel for faster feedback
- **Test Caching**: Cache test results for unchanged code
- **Test Analytics**: Track test metrics and trends
- **Test Generation**: Generate tests from specifications

## Conclusion

A well-structured test pyramid provides the foundation for confident development and reliable releases. By focusing on fast, reliable unit tests, strategic integration tests, and minimal GUI smoke tests, CuePoint can achieve high code quality while maintaining development velocity. The key is balance: enough testing to catch bugs, but not so much that tests become a burden.

The test layers work together:
- **Unit tests** catch logic errors quickly
- **Integration tests** catch workflow issues
- **GUI tests** verify the UI works at a basic level
- **Manual QA** catches edge cases and user experience issues

Together, these layers provide comprehensive coverage while maintaining fast feedback loops and development velocity.
