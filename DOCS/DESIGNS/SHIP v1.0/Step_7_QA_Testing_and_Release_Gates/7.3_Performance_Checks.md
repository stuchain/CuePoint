# Implementation Step 7.3: Performance Checks

## Implementation Overview

**What We're Building**: A comprehensive performance testing and monitoring system that establishes performance benchmarks, tracks performance metrics over time, detects performance regressions, and provides actionable insights for optimization. This system implements automated performance benchmarks for critical operations (startup, processing, filtering), establishes performance budgets and thresholds, tracks performance trends, and integrates performance checks into the CI/CD pipeline. The goal is to ensure CuePoint maintains acceptable performance characteristics, prevents performance regressions from being deployed, and provides data-driven insights for performance optimization while balancing thoroughness with execution speed.

## Strategic Context and Rationale

### Why Performance Testing Matters

Performance is a critical aspect of user experience. Poor performance leads to:
- **User Frustration**: Slow applications feel unresponsive and unprofessional
- **Lost Productivity**: Users waste time waiting for operations to complete
- **Competitive Disadvantage**: Users may switch to faster alternatives
- **Scalability Issues**: Performance problems become worse with larger datasets
- **Resource Waste**: Inefficient code wastes CPU, memory, and battery

**The Performance Testing Challenge**: Performance is complex and multi-faceted:
- **Multiple Metrics**: Response time, throughput, memory usage, CPU usage
- **Variability**: Performance varies with system load, data size, and environment
- **Trade-offs**: Optimizing one metric may degrade another
- **Context-Dependent**: What's acceptable depends on the operation and user expectations

**The Solution: Performance Budgets and Continuous Monitoring**

A well-designed performance testing strategy:
1. **Establishes Baselines**: Defines acceptable performance for each operation
2. **Tracks Trends**: Monitors performance over time to detect regressions
3. **Automates Checks**: Integrates performance checks into CI/CD pipeline
4. **Provides Insights**: Identifies bottlenecks and optimization opportunities
5. **Prevents Regressions**: Blocks deployments that degrade performance

### Industry Best Practices

**1. Performance Budgets**
- Define maximum acceptable time for each operation
- Set warning and critical thresholds
- Track performance against budgets over time

**2. Continuous Performance Testing**
- Run performance tests on every commit (or regularly)
- Track performance trends over time
- Alert on performance regressions

**3. Realistic Test Data**
- Use realistic data sizes and distributions
- Test with various data scenarios
- Include edge cases (empty data, very large data)

**4. Statistical Analysis**
- Use percentiles (p50, p95, p99) not just averages
- Account for variability in measurements
- Use statistical tests to detect regressions

**5. Performance Profiling**
- Profile code to identify bottlenecks
- Use profiling data to guide optimization
- Re-profile after optimizations to verify improvements

### CuePoint-Specific Considerations

**Application Performance Characteristics**:
- **Startup Time**: Application should start in < 2 seconds
- **Processing Speed**: 500 tracks should process in < 30 seconds
- **UI Responsiveness**: UI should remain responsive during processing
- **Filter Performance**: Table filtering should complete in < 200ms
- **Memory Usage**: Application should use < 500MB for typical operation

**Performance Critical Operations**:
1. **XML Parsing**: Rekordbox XML parsing must be fast
2. **Track Matching**: Beatport search and matching must be efficient
3. **Filtering**: Table filtering must be responsive
4. **Export Generation**: CSV/JSON export must be fast
5. **UI Updates**: Progress updates must not block UI

**Testing Challenges**:
1. **Variability**: Performance varies with system load and data
2. **Platform Differences**: Performance differs between macOS and Windows
3. **Network Dependency**: Beatport API calls affect processing time
4. **Large Datasets**: Need to test with realistic data sizes
5. **UI Responsiveness**: Hard to measure UI responsiveness automatically

**Our Strategy**:
- **Automated Benchmarks**: Run performance benchmarks in CI
- **Performance Budgets**: Define acceptable performance for each operation
- **Trend Tracking**: Track performance over time to detect regressions
- **Statistical Analysis**: Use percentiles and statistical tests
- **Manual Validation**: Manual checks for UI responsiveness

## Implementation Tasks

### Task 7.3.1: Performance Benchmark Infrastructure

**What to Build**
- Benchmark framework setup
- Performance measurement utilities
- Statistical analysis tools
- Benchmark execution infrastructure
- Results storage and tracking

**Implementation Details**

**7.3.1.1 Benchmark Framework Selection**

**Framework Choice: pytest-benchmark**

We choose pytest-benchmark for performance testing because:

1. **Integration with pytest**: Works seamlessly with existing test infrastructure
2. **Statistical Analysis**: Provides statistical analysis of benchmark results
3. **Comparison**: Can compare benchmarks across runs
4. **CI Integration**: Easy to integrate into CI/CD pipeline
5. **Reporting**: Generates detailed performance reports

**Installation**:

```bash
pip install pytest-benchmark
```

**Configuration: `pytest.ini`**

```ini
[pytest]
# Benchmark configuration
addopts = 
    --benchmark-only  # Only run benchmarks
    --benchmark-autosave  # Auto-save benchmark results
    --benchmark-save=benchmark  # Save results to file
    --benchmark-compare  # Compare with previous runs
```

**7.3.1.2 Performance Measurement Utilities**

**Purpose**: Provide utilities for measuring and analyzing performance metrics.

**Implementation: `SRC/tests/performance/benchmark_utils.py`**

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Performance benchmark utilities.

Provides utilities for measuring and analyzing performance.
"""

import time
import statistics
from contextlib import contextmanager
from typing import List, Dict, Optional
from dataclasses import dataclass


@dataclass
class PerformanceMetrics:
    """Performance metrics for an operation."""
    operation: str
    count: int
    total_time: float
    mean_time: float
    median_time: float
    min_time: float
    max_time: float
    p50_time: float
    p95_time: float
    p99_time: float
    std_dev: float


@contextmanager
def measure_time(operation_name: str):
    """Context manager to measure operation time.
    
    Usage:
        with measure_time("parse_xml") as timer:
            result = parse_xml(xml_file)
        print(f"Operation took {timer.elapsed} seconds")
    """
    start_time = time.perf_counter()
    timer = type('Timer', (), {'elapsed': 0.0})()
    
    try:
        yield timer
    finally:
        timer.elapsed = time.perf_counter() - start_time


def measure_multiple(operation, iterations: int = 10) -> List[float]:
    """Measure operation multiple times.
    
    Args:
        operation: Callable to measure
        iterations: Number of iterations
        
    Returns:
        List of execution times in seconds.
    """
    times = []
    for _ in range(iterations):
        start = time.perf_counter()
        operation()
        elapsed = time.perf_counter() - start
        times.append(elapsed)
    return times


def calculate_metrics(times: List[float], operation_name: str) -> PerformanceMetrics:
    """Calculate performance metrics from execution times.
    
    Args:
        times: List of execution times
        operation_name: Name of operation
        
    Returns:
        PerformanceMetrics object.
    """
    sorted_times = sorted(times)
    count = len(times)
    
    return PerformanceMetrics(
        operation=operation_name,
        count=count,
        total_time=sum(times),
        mean_time=statistics.mean(times),
        median_time=statistics.median(times),
        min_time=min(times),
        max_time=max(times),
        p50_time=sorted_times[int(count * 0.50)],
        p95_time=sorted_times[int(count * 0.95)],
        p99_time=sorted_times[int(count * 0.99)],
        std_dev=statistics.stdev(times) if count > 1 else 0.0
    )


def check_performance_budget(
    metrics: PerformanceMetrics,
    budget_ms: float,
    warning_threshold: Optional[float] = None,
    critical_threshold: Optional[float] = None
) -> tuple[bool, str]:
    """Check if performance meets budget.
    
    Args:
        metrics: Performance metrics
        budget_ms: Performance budget in milliseconds
        warning_threshold: Warning threshold (default: 1.2x budget)
        critical_threshold: Critical threshold (default: 1.5x budget)
        
    Returns:
        Tuple of (meets_budget, status_message).
    """
    if warning_threshold is None:
        warning_threshold = budget_ms * 1.2
    if critical_threshold is None:
        critical_threshold = budget_ms * 1.5
    
    p95_ms = metrics.p95_time * 1000
    
    if p95_ms > critical_threshold:
        return False, f"CRITICAL: {p95_ms:.1f}ms > {critical_threshold:.1f}ms (p95)"
    elif p95_ms > warning_threshold:
        return True, f"WARNING: {p95_ms:.1f}ms > {warning_threshold:.1f}ms (p95)"
    elif p95_ms > budget_ms:
        return True, f"SLOW: {p95_ms:.1f}ms > {budget_ms:.1f}ms (p95)"
    else:
        return True, f"OK: {p95_ms:.1f}ms <= {budget_ms:.1f}ms (p95)"
```

**7.3.1.3 Benchmark Test Examples**

**Example 1: XML Parsing Benchmark**

```python
# SRC/tests/performance/test_xml_parsing.py

import pytest
from tests.fixtures import get_rekordbox_fixture
from cuepoint.data.rekordbox import RekordboxParser
from tests.performance.benchmark_utils import measure_multiple, calculate_metrics, check_performance_budget


@pytest.mark.performance
class TestXMLParsingPerformance:
    """Performance benchmarks for XML parsing."""
    
    def test_parse_minimal_xml(self, benchmark):
        """Benchmark parsing minimal XML."""
        xml_path = get_rekordbox_fixture("minimal")
        parser = RekordboxParser()
        
        result = benchmark(parser.parse, xml_path)
        assert result is not None
    
    def test_parse_large_xml(self, benchmark):
        """Benchmark parsing large XML (500 tracks)."""
        xml_path = get_rekordbox_fixture("large_playlist_500_tracks")
        parser = RekordboxParser()
        
        result = benchmark(parser.parse, xml_path)
        assert len(result.tracks) == 500
    
    def test_parse_performance_budget(self):
        """Check XML parsing meets performance budget."""
        xml_path = get_rekordbox_fixture("large_playlist_500_tracks")
        parser = RekordboxParser()
        
        # Measure multiple times
        times = measure_multiple(lambda: parser.parse(xml_path), iterations=10)
        metrics = calculate_metrics(times, "parse_large_xml")
        
        # Check against budget: 500 tracks in < 2 seconds
        budget_ms = 2000.0
        meets_budget, status = check_performance_budget(metrics, budget_ms)
        
        print(f"\n{status}")
        print(f"  Mean: {metrics.mean_time*1000:.1f}ms")
        print(f"  P95: {metrics.p95_time*1000:.1f}ms")
        print(f"  P99: {metrics.p99_time*1000:.1f}ms")
        
        assert meets_budget, f"Performance budget exceeded: {status}"
```

**Example 2: Track Processing Benchmark**

```python
# SRC/tests/performance/test_processing_performance.py

import pytest
from tests.fixtures import get_rekordbox_fixture
from cuepoint.services.processor_service import ProcessorService
from cuepoint.services.bootstrap import bootstrap_services
from tests.performance.benchmark_utils import measure_multiple, calculate_metrics, check_performance_budget


@pytest.mark.performance
class TestProcessingPerformance:
    """Performance benchmarks for track processing."""
    
    @pytest.fixture
    def services(self):
        """Bootstrap services for processing."""
        return bootstrap_services()
    
    def test_process_single_track(self, benchmark, services):
        """Benchmark processing single track."""
        from cuepoint.models.track import Track
        
        track = Track(title="Test", artist="Artist", bpm=128.0)
        processor = services['processor_service']
        
        result = benchmark(processor.process_track, track)
        assert result is not None
    
    def test_process_500_tracks_budget(self, services):
        """Check processing 500 tracks meets performance budget."""
        xml_path = get_rekordbox_fixture("large_playlist_500_tracks")
        from cuepoint.data.rekordbox import RekordboxParser
        
        parser = RekordboxParser()
        rekordbox_data = parser.parse(xml_path)
        processor = services['processor_service']
        
        # Measure processing time
        def process_all():
            results = []
            for track in rekordbox_data.tracks[:500]:  # Process 500 tracks
                result = processor.process_track(track)
                results.append(result)
            return results
        
        times = measure_multiple(process_all, iterations=3)
        metrics = calculate_metrics(times, "process_500_tracks")
        
        # Budget: 500 tracks in < 30 seconds
        budget_ms = 30000.0
        meets_budget, status = check_performance_budget(metrics, budget_ms)
        
        print(f"\n{status}")
        print(f"  Mean: {metrics.mean_time:.1f}s")
        print(f"  P95: {metrics.p95_time:.1f}s")
        
        assert meets_budget, f"Performance budget exceeded: {status}"
```

**Example 3: Filter Performance Benchmark**

```python
# SRC/tests/performance/test_filter_performance.py

import pytest
from tests.performance.benchmark_utils import measure_multiple, calculate_metrics, check_performance_budget


@pytest.mark.performance
class TestFilterPerformance:
    """Performance benchmarks for table filtering."""
    
    @pytest.fixture
    def large_dataset(self):
        """Create large dataset for filtering tests."""
        from cuepoint.models.track import Track
        return [
            Track(title=f"Track {i}", artist=f"Artist {i}", bpm=120.0 + i)
            for i in range(1000)
        ]
    
    def test_filter_performance(self, benchmark, large_dataset):
        """Benchmark filtering large dataset."""
        from cuepoint.core.filter import Filter
        
        filter_obj = Filter()
        query = "Track 5"
        
        result = benchmark(filter_obj.filter, large_dataset, query)
        assert len(result) > 0
    
    def test_filter_budget(self, large_dataset):
        """Check filtering meets performance budget."""
        from cuepoint.core.filter import Filter
        
        filter_obj = Filter()
        query = "Track"
        
        def filter_operation():
            return filter_obj.filter(large_dataset, query)
        
        times = measure_multiple(filter_operation, iterations=20)
        metrics = calculate_metrics(times, "filter_1000_tracks")
        
        # Budget: Filter 1000 tracks in < 200ms
        budget_ms = 200.0
        meets_budget, status = check_performance_budget(metrics, budget_ms)
        
        print(f"\n{status}")
        print(f"  Mean: {metrics.mean_time*1000:.1f}ms")
        print(f"  P95: {metrics.p95_time*1000:.1f}ms")
        
        assert meets_budget, f"Performance budget exceeded: {status}"
```

### Task 7.3.2: Performance Regression Detection

**What to Build**
- Performance trend tracking
- Regression detection algorithms
- Historical performance data storage
- Regression alerting system
- Performance comparison utilities

**Implementation Details**

**7.3.2.1 Performance Trend Tracking**

**Purpose**: Track performance metrics over time to detect regressions and improvements.

**Storage Strategy: JSON Files**

```python
# SRC/tests/performance/performance_history.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Performance history tracking.

Stores and retrieves historical performance metrics.
"""

import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict


@dataclass
class PerformanceRecord:
    """Single performance measurement record."""
    timestamp: str
    operation: str
    metric: str  # "mean", "p95", "p99", etc.
    value: float
    unit: str  # "ms", "s", etc.
    commit_hash: Optional[str] = None
    branch: Optional[str] = None


class PerformanceHistory:
    """Track performance history over time."""
    
    def __init__(self, history_file: Path = None):
        """Initialize performance history.
        
        Args:
            history_file: Path to history file (default: .benchmarks/history.json)
        """
        if history_file is None:
            history_file = Path(".benchmarks") / "history.json"
        
        self.history_file = history_file
        self.history_file.parent.mkdir(parents=True, exist_ok=True)
        self._load_history()
    
    def _load_history(self):
        """Load history from file."""
        if self.history_file.exists():
            with open(self.history_file) as f:
                data = json.load(f)
                self.records = [PerformanceRecord(**r) for r in data]
        else:
            self.records = []
    
    def _save_history(self):
        """Save history to file."""
        data = [asdict(r) for r in self.records]
        with open(self.history_file, 'w') as f:
            json.dump(data, f, indent=2)
    
    def add_record(
        self,
        operation: str,
        metric: str,
        value: float,
        unit: str = "ms",
        commit_hash: Optional[str] = None,
        branch: Optional[str] = None
    ):
        """Add performance record.
        
        Args:
            operation: Operation name
            metric: Metric name (mean, p95, p99, etc.)
            value: Metric value
            unit: Unit (ms, s, etc.)
            commit_hash: Git commit hash
            branch: Git branch name
        """
        record = PerformanceRecord(
            timestamp=datetime.now().isoformat(),
            operation=operation,
            metric=metric,
            value=value,
            unit=unit,
            commit_hash=commit_hash,
            branch=branch
        )
        self.records.append(record)
        self._save_history()
    
    def get_history(
        self,
        operation: str,
        metric: str,
        limit: Optional[int] = None
    ) -> List[PerformanceRecord]:
        """Get performance history for operation.
        
        Args:
            operation: Operation name
            metric: Metric name
            limit: Maximum number of records to return
            
        Returns:
            List of performance records.
        """
        filtered = [
            r for r in self.records
            if r.operation == operation and r.metric == metric
        ]
        filtered.sort(key=lambda r: r.timestamp)
        
        if limit:
            return filtered[-limit:]
        return filtered
    
    def get_latest(self, operation: str, metric: str) -> Optional[PerformanceRecord]:
        """Get latest performance record.
        
        Args:
            operation: Operation name
            metric: Metric name
            
        Returns:
            Latest performance record or None.
        """
        history = self.get_history(operation, metric, limit=1)
        return history[0] if history else None
```

**7.3.2.2 Regression Detection**

**Purpose**: Detect performance regressions by comparing current performance to historical baseline.

**Implementation: `SRC/tests/performance/regression_detector.py`**

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Performance regression detection.

Detects performance regressions by comparing current metrics to historical baselines.
"""

import statistics
from typing import List, Optional, Tuple
from dataclasses import dataclass


@dataclass
class RegressionResult:
    """Result of regression detection."""
    operation: str
    metric: str
    current_value: float
    baseline_value: float
    change_percent: float
    is_regression: bool
    severity: str  # "none", "minor", "major", "critical"
    message: str


class RegressionDetector:
    """Detect performance regressions."""
    
    def __init__(self, threshold_percent: float = 10.0, critical_threshold_percent: float = 50.0):
        """Initialize regression detector.
        
        Args:
            threshold_percent: Percentage increase to consider regression (default: 10%)
            critical_threshold_percent: Percentage increase for critical regression (default: 50%)
        """
        self.threshold_percent = threshold_percent
        self.critical_threshold_percent = critical_threshold_percent
    
    def detect_regression(
        self,
        operation: str,
        metric: str,
        current_value: float,
        baseline_value: float
    ) -> RegressionResult:
        """Detect regression for operation.
        
        Args:
            operation: Operation name
            metric: Metric name
            current_value: Current metric value
            baseline_value: Baseline metric value
            
        Returns:
            RegressionResult object.
        """
        change_percent = ((current_value - baseline_value) / baseline_value) * 100
        
        if change_percent < 0:
            # Performance improved
            return RegressionResult(
                operation=operation,
                metric=metric,
                current_value=current_value,
                baseline_value=baseline_value,
                change_percent=change_percent,
                is_regression=False,
                severity="none",
                message=f"Performance improved by {abs(change_percent):.1f}%"
            )
        elif change_percent >= self.critical_threshold_percent:
            # Critical regression
            return RegressionResult(
                operation=operation,
                metric=metric,
                current_value=current_value,
                baseline_value=baseline_value,
                change_percent=change_percent,
                is_regression=True,
                severity="critical",
                message=f"CRITICAL: Performance degraded by {change_percent:.1f}% "
                       f"({current_value:.1f} vs {baseline_value:.1f})"
            )
        elif change_percent >= self.threshold_percent:
            # Regression detected
            return RegressionResult(
                operation=operation,
                metric=metric,
                current_value=current_value,
                baseline_value=baseline_value,
                change_percent=change_percent,
                is_regression=True,
                severity="major" if change_percent >= self.threshold_percent * 2 else "minor",
                message=f"Performance degraded by {change_percent:.1f}% "
                       f"({current_value:.1f} vs {baseline_value:.1f})"
            )
        else:
            # Within acceptable range
            return RegressionResult(
                operation=operation,
                metric=metric,
                current_value=current_value,
                baseline_value=baseline_value,
                change_percent=change_percent,
                is_regression=False,
                severity="none",
                message=f"Performance within acceptable range ({change_percent:.1f}% change)"
            )
    
    def calculate_baseline(self, values: List[float], method: str = "median") -> float:
        """Calculate baseline from historical values.
        
        Args:
            values: List of historical values
            method: Method to calculate baseline ("mean", "median", "p95")
            
        Returns:
            Baseline value.
        """
        if not values:
            return 0.0
        
        if method == "mean":
            return statistics.mean(values)
        elif method == "median":
            return statistics.median(values)
        elif method == "p95":
            sorted_values = sorted(values)
            index = int(len(sorted_values) * 0.95)
            return sorted_values[index]
        else:
            raise ValueError(f"Unknown baseline method: {method}")
```

**7.3.2.3 CI Integration for Regression Detection**

**Purpose**: Integrate regression detection into CI pipeline to block deployments with performance regressions.

**GitHub Actions Workflow**:

```yaml
# .github/workflows/performance.yml

name: Performance Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM

jobs:
  performance:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0  # Full history for baseline calculation
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
      
      - name: Run performance benchmarks
        run: |
          pytest SRC/tests/performance/ -m performance --benchmark-only --benchmark-autosave
      
      - name: Detect performance regressions
        run: |
          python SRC/tests/performance/check_regressions.py
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: .benchmarks/
      
      - name: Comment PR with performance results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const results = fs.readFileSync('.benchmarks/regression_report.txt', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## Performance Test Results\n\n\`\`\`\n${results}\n\`\`\``
            });
```

**Regression Check Script: `SRC/tests/performance/check_regressions.py`**

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Check for performance regressions.

Compares current benchmark results to historical baseline.
"""

import sys
from pathlib import Path
from tests.performance.performance_history import PerformanceHistory
from tests.performance.regression_detector import RegressionDetector
from tests.performance.benchmark_utils import PerformanceMetrics


def main():
    """Check for performance regressions."""
    history = PerformanceHistory()
    detector = RegressionDetector(threshold_percent=10.0, critical_threshold_percent=50.0)
    
    # Define operations to check
    operations = [
        ("parse_large_xml", "p95", 2000.0),  # 2 seconds
        ("process_500_tracks", "p95", 30000.0),  # 30 seconds
        ("filter_1000_tracks", "p95", 200.0),  # 200ms
    ]
    
    regressions = []
    
    for operation, metric, budget_ms in operations:
        # Get latest performance
        latest = history.get_latest(operation, metric)
        if not latest:
            print(f"Warning: No history for {operation}/{metric}")
            continue
        
        # Get baseline (median of last 10 runs)
        history_records = history.get_history(operation, metric, limit=10)
        if len(history_records) < 3:
            print(f"Warning: Insufficient history for {operation}/{metric}")
            continue
        
        baseline_values = [r.value for r in history_records[:-1]]  # Exclude latest
        baseline = detector.calculate_baseline(baseline_values, method="median")
        
        # Detect regression
        result = detector.detect_regression(
            operation, metric, latest.value, baseline
        )
        
        print(f"{operation}/{metric}: {result.message}")
        
        if result.is_regression:
            regressions.append(result)
    
    # Report results
    if regressions:
        print("\n" + "="*60)
        print("PERFORMANCE REGRESSIONS DETECTED")
        print("="*60)
        
        for reg in regressions:
            print(f"\n{reg.operation}/{reg.metric}:")
            print(f"  Severity: {reg.severity.upper()}")
            print(f"  {reg.message}")
        
        # Fail CI if critical regressions
        critical_regressions = [r for r in regressions if r.severity == "critical"]
        if critical_regressions:
            print("\nCRITICAL regressions detected. Failing CI.")
            sys.exit(1)
        else:
            print("\nNon-critical regressions detected. Continuing.")
            sys.exit(0)
    else:
        print("\nNo performance regressions detected.")
        sys.exit(0)


if __name__ == "__main__":
    main()
```

### Task 7.3.3: Performance Budgets and Thresholds

**What to Build**
- Performance budget definitions
- Budget validation utilities
- Budget violation detection
- Budget reporting
- Budget enforcement in CI

**Implementation Details**

**7.3.3.1 Performance Budget Definitions**

**Purpose**: Define acceptable performance characteristics for each operation.

**Performance Budgets for CuePoint**:

```python
# SRC/tests/performance/performance_budgets.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Performance budget definitions.

Defines acceptable performance characteristics for each operation.
"""

from dataclasses import dataclass
from typing import Optional


@dataclass
class PerformanceBudget:
    """Performance budget definition."""
    operation: str
    description: str
    target_ms: float
    warning_ms: Optional[float] = None
    critical_ms: Optional[float] = None
    metric: str = "p95"  # Which metric to check (mean, p50, p95, p99)
    
    def __post_init__(self):
        """Set default warning and critical thresholds."""
        if self.warning_ms is None:
            self.warning_ms = self.target_ms * 1.2
        if self.critical_ms is None:
            self.critical_ms = self.target_ms * 1.5


# Performance budgets for CuePoint operations
PERFORMANCE_BUDGETS = {
    # Startup performance
    "startup": PerformanceBudget(
        operation="startup",
        description="Application startup time",
        target_ms=2000.0,  # 2 seconds
        warning_ms=3000.0,  # 3 seconds
        critical_ms=5000.0,  # 5 seconds
    ),
    
    # XML parsing performance
    "parse_minimal_xml": PerformanceBudget(
        operation="parse_minimal_xml",
        description="Parse minimal Rekordbox XML",
        target_ms=100.0,  # 100ms
    ),
    
    "parse_large_xml": PerformanceBudget(
        operation="parse_large_xml",
        description="Parse large Rekordbox XML (500 tracks)",
        target_ms=2000.0,  # 2 seconds
    ),
    
    # Track processing performance
    "process_single_track": PerformanceBudget(
        operation="process_single_track",
        description="Process single track (with Beatport search)",
        target_ms=500.0,  # 500ms
    ),
    
    "process_500_tracks": PerformanceBudget(
        operation="process_500_tracks",
        description="Process 500 tracks",
        target_ms=30000.0,  # 30 seconds
    ),
    
    # Filtering performance
    "filter_100_tracks": PerformanceBudget(
        operation="filter_100_tracks",
        description="Filter table with 100 tracks",
        target_ms=50.0,  # 50ms
    ),
    
    "filter_1000_tracks": PerformanceBudget(
        operation="filter_1000_tracks",
        description="Filter table with 1000 tracks",
        target_ms=200.0,  # 200ms
    ),
    
    # Export performance
    "export_csv_100_tracks": PerformanceBudget(
        operation="export_csv_100_tracks",
        description="Export 100 tracks to CSV",
        target_ms=100.0,  # 100ms
    ),
    
    "export_csv_1000_tracks": PerformanceBudget(
        operation="export_csv_1000_tracks",
        description="Export 1000 tracks to CSV",
        target_ms=1000.0,  # 1 second
    ),
    
    # UI responsiveness
    "ui_response": PerformanceBudget(
        operation="ui_response",
        description="UI response time for user actions",
        target_ms=100.0,  # 100ms
        warning_ms=200.0,  # 200ms
        critical_ms=500.0,  # 500ms
    ),
}
```

**7.3.3.2 Budget Validation**

**Purpose**: Validate that performance meets defined budgets.

**Implementation**:

```python
# SRC/tests/performance/budget_validator.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Performance budget validation.

Validates that performance meets defined budgets.
"""

from typing import List, Tuple
from tests.performance.performance_budgets import PERFORMANCE_BUDGETS, PerformanceBudget
from tests.performance.benchmark_utils import PerformanceMetrics


class BudgetValidator:
    """Validate performance against budgets."""
    
    def validate(
        self,
        operation: str,
        metrics: PerformanceMetrics
    ) -> Tuple[bool, str]:
        """Validate performance against budget.
        
        Args:
            operation: Operation name
            metrics: Performance metrics
            
        Returns:
            Tuple of (meets_budget, status_message).
        """
        if operation not in PERFORMANCE_BUDGETS:
            return True, f"No budget defined for {operation}"
        
        budget = PERFORMANCE_BUDGETS[operation]
        
        # Get metric value based on budget.metric
        if budget.metric == "mean":
            value_ms = metrics.mean_time * 1000
        elif budget.metric == "median":
            value_ms = metrics.median_time * 1000
        elif budget.metric == "p50":
            value_ms = metrics.p50_time * 1000
        elif budget.metric == "p95":
            value_ms = metrics.p95_time * 1000
        elif budget.metric == "p99":
            value_ms = metrics.p99_time * 1000
        else:
            return True, f"Unknown metric: {budget.metric}"
        
        # Check against thresholds
        if value_ms > budget.critical_ms:
            return False, (
                f"CRITICAL: {operation} exceeds critical threshold\n"
                f"  {value_ms:.1f}ms > {budget.critical_ms:.1f}ms ({budget.metric})\n"
                f"  Budget: {budget.target_ms:.1f}ms"
            )
        elif value_ms > budget.warning_ms:
            return True, (
                f"WARNING: {operation} exceeds warning threshold\n"
                f"  {value_ms:.1f}ms > {budget.warning_ms:.1f}ms ({budget.metric})\n"
                f"  Budget: {budget.target_ms:.1f}ms"
            )
        elif value_ms > budget.target_ms:
            return True, (
                f"SLOW: {operation} exceeds target\n"
                f"  {value_ms:.1f}ms > {budget.target_ms:.1f}ms ({budget.metric})"
            )
        else:
            return True, (
                f"OK: {operation} meets budget\n"
                f"  {value_ms:.1f}ms <= {budget.target_ms:.1f}ms ({budget.metric})"
            )
    
    def validate_all(self, results: dict) -> List[Tuple[str, bool, str]]:
        """Validate all operations against budgets.
        
        Args:
            results: Dictionary mapping operation names to PerformanceMetrics
            
        Returns:
            List of (operation, meets_budget, status_message) tuples.
        """
        validations = []
        for operation, metrics in results.items():
            meets_budget, message = self.validate(operation, metrics)
            validations.append((operation, meets_budget, message))
        return validations
```

## Integration Points

### CI/CD Integration

Performance checks are integrated into the CI pipeline:
- Run performance benchmarks on every commit
- Compare to historical baseline
- Block deployments with critical regressions
- Report performance trends

### Development Workflow

Developers can run performance checks locally:
```bash
# Run performance benchmarks
pytest SRC/tests/performance/ -m performance

# Check for regressions
python SRC/tests/performance/check_regressions.py

# Validate against budgets
python SRC/tests/performance/validate_budgets.py
```

## Success Metrics

### Performance Goals

- **Startup**: < 2 seconds
- **Processing**: 500 tracks in < 30 seconds
- **Filtering**: 1000 tracks in < 200ms
- **UI Response**: < 100ms for user actions

### Quality Goals

- **Regression Detection**: > 90% of regressions caught before deployment
- **False Positives**: < 5% false positive rate
- **Test Reliability**: Performance tests are stable and repeatable

## Future Considerations

### Advanced Performance Testing

- **Load Testing**: Test under various system loads
- **Stress Testing**: Test with extreme data sizes
- **Memory Profiling**: Track memory usage over time
- **CPU Profiling**: Identify CPU bottlenecks

### Performance Optimization

- **Profiling Integration**: Use profiling data to guide optimization
- **A/B Testing**: Compare performance of different implementations
- **Automated Optimization**: Suggest optimizations based on performance data

## Conclusion

Performance testing is essential for maintaining a responsive, efficient application. By establishing performance budgets, tracking performance trends, detecting regressions, and integrating performance checks into the CI/CD pipeline, CuePoint can ensure consistent performance while enabling data-driven optimization decisions.

The key is balance: enough performance testing to catch regressions and guide optimization, but not so much that it becomes a burden. With automated benchmarks, regression detection, and budget validation, performance becomes a first-class concern that's continuously monitored and maintained.
