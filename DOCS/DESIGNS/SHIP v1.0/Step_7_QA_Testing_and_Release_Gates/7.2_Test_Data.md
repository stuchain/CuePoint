# Implementation Step 7.2: Test Data

## Implementation Overview

**What We're Building**: A comprehensive test data management system that provides reliable, deterministic fixtures for all testing scenarios. This system implements a "record once, replay always" strategy for network-dependent tests, curates XML and HTML snapshots for parsing and integration tests, maintains golden files for output validation, and establishes clear patterns for fixture management, versioning, and maintenance. The goal is to create a test data infrastructure that enables fast, reliable, repeatable tests while avoiding dependencies on external services, network conditions, or time-sensitive data.

## Strategic Context and Rationale

### Why Test Data Management Matters

Test data is the foundation of reliable testing. Poor test data management leads to:
- **Flaky Tests**: Tests that pass or fail randomly due to external dependencies
- **Slow Tests**: Tests that wait for network timeouts or external services
- **Brittle Tests**: Tests that break when external data changes
- **Maintenance Burden**: Tests that require constant updates as external data evolves
- **CI Failures**: Tests that fail in CI due to network issues or unavailable services

**The Test Data Challenge**: Every application needs test data, but the requirements vary:
- **Static Data**: Simple constants or small datasets
- **Dynamic Data**: Generated data that varies between test runs
- **External Data**: Data from APIs, databases, or file systems
- **Time-Sensitive Data**: Data that changes over time (timestamps, dates)
- **Large Data**: Datasets that are too large to include in the repository

**The Solution: Fixture Strategy**

A well-designed fixture strategy addresses all these challenges:
1. **Deterministic Fixtures**: Predictable, repeatable test data
2. **Versioned Fixtures**: Track changes to test data over time
3. **Minimal Fixtures**: Small, focused datasets that test specific scenarios
4. **Recorded Responses**: Capture external API responses for replay
5. **Golden Files**: Expected outputs for validation

### Industry Best Practices

**1. VCR (Video Cassette Recorder) Pattern**
- Record HTTP interactions once
- Replay recorded interactions in tests
- Popular libraries: VCR (Ruby), Betamax (Python), WireMock (Java)

**2. Golden File Testing**
- Store expected outputs as "golden files"
- Compare actual outputs to golden files
- Update golden files intentionally when behavior changes

**3. Test Data Builders**
- Use builder pattern to construct test data
- Provide sensible defaults
- Allow customization for specific test scenarios

**4. Fixture Factories**
- Generate test data programmatically
- Use factories for complex object creation
- Provide factories for common scenarios

**5. Snapshot Testing**
- Capture snapshots of complex data structures
- Compare snapshots in tests
- Update snapshots when behavior intentionally changes

### CuePoint-Specific Considerations

**Application Data Types**:
- **Rekordbox XML**: Large XML files with playlists and tracks
- **Beatport HTML**: HTML responses from Beatport search pages
- **CSV Exports**: Expected output files for validation
- **Track Data**: Structured track information (title, artist, BPM, etc.)
- **Playlist Data**: Collections of tracks with metadata

**Testing Challenges**:
1. **Large XML Files**: Rekordbox XML files can be several MB
2. **HTML Parsing**: Beatport HTML structure may change
3. **Network Dependency**: Beatport API requires network access
4. **Data Evolution**: Track data formats may change over time
5. **Platform Differences**: Some data may differ between platforms

**Our Strategy**:
- **Small, Focused Fixtures**: Use minimal XML/HTML that covers test scenarios
- **Recorded Responses**: Record Beatport HTML responses once, replay always
- **Golden Files**: Store expected CSV outputs for validation
- **Versioned Fixtures**: Track fixture versions and update intentionally
- **Fixture Factories**: Generate test data programmatically when possible

## Implementation Tasks

### Task 7.2.1: Fixture Strategy and Organization

**What to Build**
- Fixture directory structure
- Fixture naming conventions
- Fixture versioning strategy
- Fixture documentation
- Fixture loading utilities

**Implementation Details**

**7.2.1.1 Fixture Directory Structure**

**Purpose**: Organize test fixtures in a clear, maintainable structure that scales with the project.

**Directory Layout**:

```
SRC/tests/
├── fixtures/
│   ├── __init__.py
│   ├── README.md                    # Fixture documentation
│   ├── rekordbox/                   # Rekordbox XML fixtures
│   │   ├── minimal.xml              # Minimal valid XML
│   │   ├── single_playlist.xml      # One playlist, few tracks
│   │   ├── multiple_playlists.xml   # Multiple playlists
│   │   ├── large_playlist.xml       # Large playlist (500+ tracks)
│   │   └── edge_cases/              # Edge case fixtures
│   │       ├── empty_playlist.xml
│   │       ├── no_tracks.xml
│   │       └── invalid_structure.xml
│   ├── beatport/                    # Beatport HTML fixtures
│   │   ├── search_results.html       # Standard search results
│   │   ├── no_results.html           # Empty search results
│   │   ├── single_result.html        # Single track result
│   │   └── edge_cases/               # Edge case fixtures
│   │       ├── malformed_html.html
│   │       └── timeout_response.html
│   ├── exports/                     # Golden files (expected outputs)
│   │   ├── csv/
│   │   │   ├── minimal_export.csv
│   │   │   ├── full_export.csv
│   │   │   └── edge_cases/
│   │   └── json/
│   │       └── minimal_export.json
│   └── tracks/                       # Track data fixtures
│       ├── sample_tracks.json
│       └── edge_cases/
│           ├── missing_fields.json
│           └── unicode_tracks.json
```

**7.2.1.2 Fixture Naming Conventions**

**Purpose**: Establish clear naming conventions that make fixtures self-documenting and easy to discover.

**Naming Patterns**:

1. **Descriptive Names**: Names should describe the fixture's purpose
   - ✅ `minimal_valid_xml.xml` - Clear purpose
   - ❌ `test1.xml` - Unclear purpose

2. **Scenario-Based Names**: Names should indicate the test scenario
   - ✅ `single_playlist_10_tracks.xml` - Specific scenario
   - ❌ `playlist.xml` - Too generic

3. **Edge Case Suffixes**: Use `_edge_case` or `_invalid` for edge cases
   - ✅ `empty_playlist_edge_case.xml`
   - ✅ `malformed_html_invalid.html`

4. **Version Suffixes**: Use version suffixes when fixtures evolve
   - ✅ `search_results_v1.html`
   - ✅ `search_results_v2.html` (if structure changes)

**Example Fixture Names**:

```
rekordbox/
├── minimal_valid.xml                    # Minimal valid Rekordbox XML
├── single_playlist_5_tracks.xml         # One playlist with 5 tracks
├── multiple_playlists_3_each.xml       # 3 playlists, 3 tracks each
├── large_playlist_500_tracks.xml        # Large playlist for performance
├── edge_cases/
│   ├── empty_playlist.xml               # Playlist with no tracks
│   ├── no_playlists.xml                 # XML with no playlists
│   ├── invalid_xml_structure.xml        # Invalid XML structure
│   ├── missing_required_fields.xml      # Missing required track fields
│   └── unicode_track_names.xml           # Tracks with Unicode characters

beatport/
├── search_results_standard.html         # Standard search results page
├── search_results_no_matches.html        # No search results
├── search_results_single_match.html      # Single result
├── search_results_many_matches.html      # Many results (pagination)
└── edge_cases/
    ├── malformed_html.html               # Invalid HTML structure
    ├── missing_track_info.html           # Missing track information
    └── timeout_response.html             # Simulated timeout

exports/
├── csv/
│   ├── minimal_export_5_tracks.csv       # Minimal CSV export
│   ├── full_export_all_fields.csv        # Full export with all fields
│   └── edge_cases/
│       ├── empty_export.csv               # Empty export
│       └── unicode_export.csv             # Export with Unicode
└── json/
    └── minimal_export_5_tracks.json      # Minimal JSON export
```

**7.2.1.3 Fixture Loading Utilities**

**Purpose**: Provide convenient utilities for loading fixtures in tests, reducing boilerplate and improving test readability.

**Implementation: `SRC/tests/fixtures/__init__.py`**

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Test fixture utilities and helpers.

Provides convenient functions for loading test fixtures in tests.
"""

from pathlib import Path
from typing import Optional


# Base fixture directory
FIXTURE_DIR = Path(__file__).parent


def get_fixture_path(relative_path: str) -> Path:
    """Get absolute path to a fixture file.
    
    Args:
        relative_path: Path relative to fixtures directory (e.g., "rekordbox/minimal.xml")
        
    Returns:
        Absolute path to fixture file.
        
    Raises:
        FileNotFoundError: If fixture file doesn't exist.
    """
    fixture_path = FIXTURE_DIR / relative_path
    if not fixture_path.exists():
        raise FileNotFoundError(
            f"Fixture not found: {relative_path}\n"
            f"Expected at: {fixture_path}"
        )
    return fixture_path


def load_fixture(relative_path: str, encoding: str = "utf-8") -> str:
    """Load fixture file as text.
    
    Args:
        relative_path: Path relative to fixtures directory
        encoding: Text encoding (default: utf-8)
        
    Returns:
        File contents as string.
    """
    fixture_path = get_fixture_path(relative_path)
    return fixture_path.read_text(encoding=encoding)


def load_fixture_bytes(relative_path: str) -> bytes:
    """Load fixture file as bytes.
    
    Args:
        relative_path: Path relative to fixtures directory
        
    Returns:
        File contents as bytes.
    """
    fixture_path = get_fixture_path(relative_path)
    return fixture_path.read_bytes()


# Convenience functions for common fixtures
def get_rekordbox_fixture(name: str) -> Path:
    """Get path to Rekordbox XML fixture.
    
    Args:
        name: Fixture name (e.g., "minimal", "single_playlist")
        
    Returns:
        Path to fixture file.
    """
    return get_fixture_path(f"rekordbox/{name}.xml")


def get_beatport_fixture(name: str) -> Path:
    """Get path to Beatport HTML fixture.
    
    Args:
        name: Fixture name (e.g., "search_results_standard")
        
    Returns:
        Path to fixture file.
    """
    return get_fixture_path(f"beatport/{name}.html")


def get_export_fixture(name: str, format: str = "csv") -> Path:
    """Get path to export golden file.
    
    Args:
        name: Fixture name (e.g., "minimal_export")
        format: Export format (csv, json)
        
    Returns:
        Path to fixture file.
    """
    return get_fixture_path(f"exports/{format}/{name}.{format}")


# Pytest fixtures for common test data
def pytest_configure(config):
    """Register fixture markers."""
    config.addinivalue_line(
        "markers", "fixture: Test uses fixtures"
    )
```

**Usage in Tests**:

```python
# SRC/tests/unit/data/test_rekordbox.py

import pytest
from tests.fixtures import get_rekordbox_fixture, load_fixture


@pytest.mark.unit
class TestRekordboxParser:
    """Test Rekordbox XML parser with fixtures."""
    
    def test_parse_minimal_xml(self):
        """Test parsing minimal valid XML."""
        xml_path = get_rekordbox_fixture("minimal")
        parser = RekordboxParser()
        result = parser.parse(xml_path)
        
        assert len(result.tracks) > 0
    
    def test_parse_large_playlist(self):
        """Test parsing large playlist."""
        xml_path = get_rekordbox_fixture("large_playlist_500_tracks")
        parser = RekordboxParser()
        result = parser.parse(xml_path)
        
        assert len(result.tracks) == 500
    
    def test_parse_edge_case_empty_playlist(self):
        """Test parsing empty playlist."""
        xml_path = get_rekordbox_fixture("edge_cases/empty_playlist")
        parser = RekordboxParser()
        result = parser.parse(xml_path)
        
        assert len(result.playlists) == 1
        assert len(result.playlists[0].tracks) == 0
```

### Task 7.2.2: Rekordbox XML Fixtures

**What to Build**
- Minimal valid XML fixture
- Single playlist fixture
- Multiple playlists fixture
- Large playlist fixture (for performance testing)
- Edge case fixtures (empty, invalid, etc.)

**Implementation Details**

**7.2.2.1 Minimal Valid XML Fixture**

**Purpose**: Provide the smallest valid Rekordbox XML that can be used for basic parsing tests.

**File: `SRC/tests/fixtures/rekordbox/minimal.xml`**

```xml
<?xml version="1.0" encoding="UTF-8"?>
<DJ_PLAYLISTS Version="1.0.0">
    <PRODUCT Name="rekordbox" Version="6.7.0"/>
    <COLLECTION>
        <TRACK 
            TrackID="1" 
            Name="Test Track 1" 
            Artist="Test Artist 1" 
            BPM="128.0"
            Key="Am"
            Genre="House"
            Year="2024"
        />
    </COLLECTION>
    <PLAYLISTS>
        <NODE Name="ROOT">
            <NODE Name="Test Playlist">
                <TRACK Key="1"/>
            </NODE>
        </NODE>
    </PLAYLISTS>
</DJ_PLAYLISTS>
```

**Characteristics**:
- Single track in collection
- Single playlist with one track
- All required fields present
- Valid XML structure
- Size: ~500 bytes

**7.2.2.2 Single Playlist Fixture**

**Purpose**: Provide a realistic single playlist with multiple tracks for integration testing.

**File: `SRC/tests/fixtures/rekordbox/single_playlist_10_tracks.xml`**

```xml
<?xml version="1.0" encoding="UTF-8"?>
<DJ_PLAYLISTS Version="1.0.0">
    <PRODUCT Name="rekordbox" Version="6.7.0"/>
    <COLLECTION>
        <!-- 10 tracks with varied data -->
        <TRACK TrackID="1" Name="Track 1" Artist="Artist 1" BPM="128.0" Key="Am" Genre="House" Year="2024"/>
        <TRACK TrackID="2" Name="Track 2" Artist="Artist 2" BPM="130.0" Key="C" Genre="Techno" Year="2023"/>
        <TRACK TrackID="3" Name="Track 3" Artist="Artist 3" BPM="125.0" Key="Dm" Genre="Deep House" Year="2024"/>
        <!-- ... 7 more tracks ... -->
    </COLLECTION>
    <PLAYLISTS>
        <NODE Name="ROOT">
            <NODE Name="My Playlist">
                <TRACK Key="1"/>
                <TRACK Key="2"/>
                <TRACK Key="3"/>
                <!-- ... 7 more tracks ... -->
            </NODE>
        </NODE>
    </PLAYLISTS>
</DJ_PLAYLISTS>
```

**Characteristics**:
- 10 tracks with varied metadata
- Single playlist containing all tracks
- Realistic track data (different BPMs, keys, genres)
- Size: ~2 KB

**7.2.2.3 Large Playlist Fixture**

**Purpose**: Provide a large playlist for performance testing and stress testing.

**File: `SRC/tests/fixtures/rekordbox/large_playlist_500_tracks.xml`**

**Characteristics**:
- 500 tracks in collection
- Single playlist with all 500 tracks
- Generated programmatically (not hand-written)
- Size: ~100 KB

**Generation Script: `SRC/tests/fixtures/rekordbox/generate_large_fixture.py`**

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Generate large Rekordbox XML fixture for performance testing.
"""

from pathlib import Path


def generate_large_xml(num_tracks: int = 500, output_path: Path = None):
    """Generate large Rekordbox XML fixture.
    
    Args:
        num_tracks: Number of tracks to generate
        output_path: Output file path
    """
    if output_path is None:
        output_path = Path(__file__).parent / f"large_playlist_{num_tracks}_tracks.xml"
    
    # Generate XML content
    tracks_xml = []
    track_keys = []
    
    for i in range(1, num_tracks + 1):
        bpm = 120.0 + (i % 20)  # Vary BPM between 120-140
        key = ["Am", "C", "Dm", "F", "G"][i % 5]  # Cycle through keys
        genre = ["House", "Techno", "Deep House", "Progressive"][i % 4]
        year = 2020 + (i % 5)  # Years 2020-2024
        
        tracks_xml.append(
            f'        <TRACK TrackID="{i}" Name="Track {i}" '
            f'Artist="Artist {i}" BPM="{bpm}" Key="{key}" '
            f'Genre="{genre}" Year="{year}"/>'
        )
        track_keys.append(f'                <TRACK Key="{i}"/>')
    
    xml_content = f"""<?xml version="1.0" encoding="UTF-8"?>
<DJ_PLAYLISTS Version="1.0.0">
    <PRODUCT Name="rekordbox" Version="6.7.0"/>
    <COLLECTION>
{chr(10).join(tracks_xml)}
    </COLLECTION>
    <PLAYLISTS>
        <NODE Name="ROOT">
            <NODE Name="Large Playlist">
{chr(10).join(track_keys)}
            </NODE>
        </NODE>
    </PLAYLISTS>
</DJ_PLAYLISTS>"""
    
    output_path.write_text(xml_content, encoding="utf-8")
    print(f"Generated {num_tracks} track fixture: {output_path}")


if __name__ == "__main__":
    generate_large_xml(500)
```

**7.2.2.4 Edge Case Fixtures**

**Purpose**: Test parser behavior with edge cases and invalid data.

**Edge Cases to Cover**:

1. **Empty Playlist**: Playlist with no tracks
2. **No Playlists**: XML with no playlists
3. **Invalid XML**: Malformed XML structure
4. **Missing Fields**: Tracks with missing required fields
5. **Unicode**: Tracks with Unicode characters
6. **Special Characters**: Tracks with special characters in names

**Example: Empty Playlist**

```xml
<?xml version="1.0" encoding="UTF-8"?>
<DJ_PLAYLISTS Version="1.0.0">
    <PRODUCT Name="rekordbox" Version="6.7.0"/>
    <COLLECTION>
        <TRACK TrackID="1" Name="Track 1" Artist="Artist 1" BPM="128.0"/>
    </COLLECTION>
    <PLAYLISTS>
        <NODE Name="ROOT">
            <NODE Name="Empty Playlist">
                <!-- No tracks -->
            </NODE>
        </NODE>
    </PLAYLISTS>
</DJ_PLAYLISTS>
```

### Task 7.2.3: Beatport HTML Fixtures

**What to Build**
- Recorded Beatport HTML responses
- Standard search results fixture
- Edge case fixtures (no results, single result, etc.)
- Recording and replay utilities
- Version management for HTML fixtures

**Implementation Details**

**7.2.3.1 Recording Strategy: "Record Once, Replay Always"**

**Purpose**: Capture real Beatport HTML responses once, then replay them in all tests to avoid network dependencies.

**Recording Process**:

1. **Initial Recording**: Make real Beatport API calls and save HTML responses
2. **Version Control**: Commit recorded responses to repository
3. **Replay in Tests**: Use recorded responses instead of real API calls
4. **Update When Needed**: Re-record when Beatport HTML structure changes

**Recording Script: `SRC/tests/fixtures/beatport/record_responses.py`**

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Record Beatport HTML responses for use in tests.

Run this script to capture real Beatport responses and save them as fixtures.
"""

import requests
from pathlib import Path
from datetime import datetime


FIXTURE_DIR = Path(__file__).parent


def record_search_response(query: str, fixture_name: str):
    """Record Beatport search response.
    
    Args:
        query: Search query
        fixture_name: Name for fixture file
    """
    # Make real request to Beatport
    url = f"https://www.beatport.com/search?q={query}"
    response = requests.get(url)
    response.raise_for_status()
    
    # Save HTML response
    fixture_path = FIXTURE_DIR / f"{fixture_name}.html"
    fixture_path.write_text(response.text, encoding="utf-8")
    
    # Save metadata
    metadata_path = FIXTURE_DIR / f"{fixture_name}.meta.json"
    metadata = {
        "url": url,
        "query": query,
        "recorded_at": datetime.now().isoformat(),
        "status_code": response.status_code,
        "content_length": len(response.text)
    }
    import json
    metadata_path.write_text(json.dumps(metadata, indent=2))
    
    print(f"Recorded: {fixture_name}")


def main():
    """Record common search scenarios."""
    # Standard search with results
    record_search_response("house music", "search_results_standard")
    
    # Search with no results
    record_search_response("nonexistenttrackxyz123", "search_results_no_matches")
    
    # Search with single result
    record_search_response("specific track name", "search_results_single_match")
    
    print("Recording complete!")


if __name__ == "__main__":
    main()
```

**7.2.3.2 Standard Search Results Fixture**

**Purpose**: Provide a realistic Beatport search results page for testing HTML parsing.

**File: `SRC/tests/fixtures/beatport/search_results_standard.html`**

**Characteristics**:
- Real Beatport HTML structure
- Multiple search results
- All track information present (title, artist, label, BPM, etc.)
- Recorded from real Beatport search
- Size: ~50-100 KB

**Usage in Tests**:

```python
# SRC/tests/integration/test_beatport_integration.py

import pytest
from unittest.mock import patch, Mock
from tests.fixtures import get_beatport_fixture, load_fixture
from cuepoint.services.beatport_service import BeatportService


@pytest.mark.integration
class TestBeatportIntegration:
    """Integration tests using recorded Beatport HTML."""
    
    def test_parse_search_results(self):
        """Test parsing recorded search results."""
        # Load recorded HTML
        html_content = load_fixture("beatport/search_results_standard.html")
        
        # Mock HTTP request to return recorded HTML
        with patch('requests.get') as mock_get:
            mock_response = Mock()
            mock_response.text = html_content
            mock_response.status_code = 200
            mock_get.return_value = mock_response
            
            # Test parsing
            service = BeatportService()
            results = service.search("house music")
            
            assert len(results) > 0
            assert all(r.title for r in results)
            assert all(r.artist for r in results)
```

**7.2.3.3 Edge Case Fixtures**

**Edge Cases to Cover**:

1. **No Results**: Empty search results page
2. **Single Result**: Search with only one match
3. **Many Results**: Search with pagination
4. **Malformed HTML**: Invalid HTML structure
5. **Missing Information**: HTML with missing track data
6. **Timeout Response**: Simulated timeout response

### Task 7.2.4: Golden Files for Output Validation

**What to Build**
- Expected CSV export files
- Expected JSON export files
- Golden file comparison utilities
- Golden file update process
- Version management for golden files

**Implementation Details**

**7.2.4.1 Golden File Strategy**

**Purpose**: Store expected outputs ("golden files") to validate that exports match expected formats and content.

**Golden File Philosophy**:

1. **Version Control**: Golden files are committed to repository
2. **Intentional Updates**: Golden files are updated when behavior intentionally changes
3. **Comparison**: Tests compare actual outputs to golden files
4. **Documentation**: Golden files serve as documentation of expected output format

**7.2.4.2 CSV Export Golden Files**

**File: `SRC/tests/fixtures/exports/csv/minimal_export_5_tracks.csv`**

```csv
Title,Artist,Label,BPM,Key,Genre,Year,Match Score
Track 1,Artist 1,Label 1,128.0,Am,House,2024,0.95
Track 2,Artist 2,Label 2,130.0,C,Techno,2023,0.92
Track 3,Artist 3,Label 3,125.0,Dm,Deep House,2024,0.88
Track 4,Artist 4,Label 4,132.0,F,Progressive,2023,0.85
Track 5,Artist 5,Label 5,128.0,G,House,2024,0.90
```

**Usage in Tests**:

```python
# SRC/tests/integration/test_export_integration.py

import pytest
from pathlib import Path
from tests.fixtures import get_export_fixture, load_fixture
from cuepoint.services.export_service import ExportService


@pytest.mark.integration
class TestExportIntegration:
    """Integration tests for export functionality."""
    
    def test_csv_export_matches_golden(self, temp_dir: Path, sample_tracks):
        """Test that CSV export matches golden file."""
        # Generate export
        export_service = ExportService()
        export_path = temp_dir / "export.csv"
        export_service.export_to_csv(sample_tracks, export_path)
        
        # Load golden file
        golden_path = get_export_fixture("minimal_export_5_tracks", "csv")
        golden_content = load_fixture("exports/csv/minimal_export_5_tracks.csv")
        
        # Compare (normalize line endings)
        actual_content = export_path.read_text().replace("\r\n", "\n")
        expected_content = golden_content.replace("\r\n", "\n")
        
        assert actual_content == expected_content, \
            f"Export does not match golden file.\n" \
            f"Expected:\n{expected_content}\n" \
            f"Actual:\n{actual_content}"
```

**7.2.4.3 Golden File Update Process**

**Purpose**: Establish a clear process for updating golden files when behavior intentionally changes.

**Update Workflow**:

1. **Run Tests**: Tests fail because output doesn't match golden file
2. **Verify Change is Intentional**: Confirm the change is expected, not a bug
3. **Update Golden File**: Update golden file with new expected output
4. **Commit Change**: Commit updated golden file with explanation
5. **Verify Tests Pass**: Confirm tests pass with updated golden file

**Update Script: `SRC/tests/fixtures/exports/update_golden.py`**

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Update golden files with new expected outputs.

Use this script when export format intentionally changes.
"""

import sys
from pathlib import Path


def update_golden(actual_output: Path, golden_path: Path):
    """Update golden file with actual output.
    
    Args:
        actual_output: Path to actual output file
        golden_path: Path to golden file to update
    """
    if not actual_output.exists():
        print(f"Error: Actual output not found: {actual_output}")
        return False
    
    # Copy actual output to golden file
    golden_path.parent.mkdir(parents=True, exist_ok=True)
    golden_path.write_bytes(actual_output.read_bytes())
    
    print(f"Updated golden file: {golden_path}")
    return True


if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: update_golden.py <actual_output> <golden_path>")
        sys.exit(1)
    
    actual = Path(sys.argv[1])
    golden = Path(sys.argv[2])
    update_golden(actual, golden)
```

### Task 7.2.5: Fixture Maintenance and Documentation

**What to Build**
- Fixture documentation
- Fixture versioning strategy
- Fixture update procedures
- Fixture validation utilities
- Fixture size management

**Implementation Details**

**7.2.5.1 Fixture Documentation**

**File: `SRC/tests/fixtures/README.md`**

```markdown
# Test Fixtures

This directory contains test fixtures used across all test layers.

## Directory Structure

- `rekordbox/`: Rekordbox XML fixtures
- `beatport/`: Beatport HTML response fixtures
- `exports/`: Golden files (expected outputs)
- `tracks/`: Track data fixtures

## Fixture Naming

Fixtures follow these naming conventions:
- Descriptive names that indicate purpose
- Scenario-based names for specific test cases
- `_edge_case` suffix for edge cases
- Version suffixes when fixtures evolve

## Recording Beatport Responses

To record new Beatport HTML responses:

```bash
python SRC/tests/fixtures/beatport/record_responses.py
```

This will make real API calls and save responses as fixtures.

## Updating Golden Files

When export format intentionally changes:

```bash
python SRC/tests/fixtures/exports/update_golden.py <actual_output> <golden_path>
```

## Fixture Guidelines

1. **Keep Fixtures Small**: Use minimal data that covers test scenarios
2. **Version Control**: All fixtures are committed to repository
3. **Document Changes**: Document why fixtures were updated
4. **Validate Fixtures**: Ensure fixtures are valid before committing
```

**7.2.5.2 Fixture Validation**

**Purpose**: Validate fixtures to ensure they're valid and usable.

**Validation Script: `SRC/tests/fixtures/validate_fixtures.py`**

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Validate all test fixtures.

Ensures fixtures are valid and can be loaded correctly.
"""

from pathlib import Path
import xml.etree.ElementTree as ET


def validate_rekordbox_fixtures():
    """Validate Rekordbox XML fixtures."""
    fixture_dir = Path(__file__).parent / "rekordbox"
    errors = []
    
    for xml_file in fixture_dir.glob("*.xml"):
        try:
            tree = ET.parse(xml_file)
            root = tree.getroot()
            
            # Basic validation
            if root.tag != "DJ_PLAYLISTS":
                errors.append(f"{xml_file}: Invalid root element")
            
        except ET.ParseError as e:
            errors.append(f"{xml_file}: XML parse error: {e}")
        except Exception as e:
            errors.append(f"{xml_file}: Error: {e}")
    
    return errors


def main():
    """Validate all fixtures."""
    print("Validating fixtures...")
    
    errors = []
    errors.extend(validate_rekordbox_fixtures())
    
    if errors:
        print("Validation errors:")
        for error in errors:
            print(f"  - {error}")
        return 1
    else:
        print("All fixtures valid!")
        return 0


if __name__ == "__main__":
    exit(main())
```

## Integration Points

### CI/CD Integration

**Fixture Validation in CI**:

```yaml
# .github/workflows/test.yml

- name: Validate Fixtures
  run: |
    python SRC/tests/fixtures/validate_fixtures.py
```

### Development Workflow

**Pre-commit Hook for Fixture Validation**:

```yaml
# .pre-commit-config.yaml

- repo: local
  hooks:
    - id: validate-fixtures
      name: Validate test fixtures
      entry: python
      args: [SRC/tests/fixtures/validate_fixtures.py]
      language: system
      files: SRC/tests/fixtures/.*
```

## Success Metrics

### Fixture Quality Metrics

- **Coverage**: All test scenarios have fixtures
- **Validity**: All fixtures pass validation
- **Size**: Fixtures are appropriately sized (< 1 MB each)
- **Maintenance**: Fixtures are updated when needed

### Test Reliability Metrics

- **Determinism**: Tests are 100% deterministic with fixtures
- **Speed**: Tests complete quickly without network delays
- **Stability**: Tests don't fail due to external data changes

## Future Considerations

### Fixture Management Improvements

- **Fixture Generation**: Generate fixtures programmatically
- **Fixture Compression**: Compress large fixtures
- **Fixture Caching**: Cache parsed fixtures in memory
- **Fixture Versioning**: Track fixture versions and changes

### Advanced Fixture Strategies

- **Parameterized Fixtures**: Generate fixtures with parameters
- **Fixture Factories**: Create fixtures on-demand
- **Fixture Sharing**: Share fixtures across test modules
- **Fixture Documentation**: Auto-generate fixture documentation

## Conclusion

A well-designed fixture strategy is essential for reliable, fast, and maintainable tests. By implementing a "record once, replay always" strategy for network-dependent tests, curating focused XML and HTML fixtures, maintaining golden files for output validation, and establishing clear fixture management practices, CuePoint can achieve high test reliability while avoiding external dependencies and network-related test failures.

The key principles are:
- **Determinism**: Tests must be repeatable and predictable
- **Minimalism**: Use the smallest fixtures that cover test scenarios
- **Versioning**: Track fixture changes and update intentionally
- **Documentation**: Document fixture purpose and usage
- **Validation**: Ensure fixtures are valid and usable

With these principles in place, the test data infrastructure becomes a solid foundation for comprehensive testing.
